{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "root_folder =\"C:/Users/adity/OneDrive/Desktop/Sixth Semester/CE6018 Seismic Data Analytics/\"\n",
    "# root_folder =\"C:/Users/user/Desktop/Adi/GMM/\"\n",
    "\n",
    "folder_path = root_folder+\"Program/Week8/PBNN/EPSD plot/EPSD_Data/\"\n",
    "num_epochs = 2000\n",
    "# Load the data\n",
    "freqEnv_actual = pd.read_csv(folder_path+f'FreqEnv/Actuals_epoch{num_epochs}.csv')\n",
    "freqEnv_prediction = pd.read_csv(\n",
    "    folder_path+f'FreqEnv/Predictions_epoch{num_epochs}.csv')\n",
    "\n",
    "\n",
    "timeEnv_actual = pd.read_csv(folder_path+f'TimeEnv/Actuals_epoch{num_epochs}.csv')\n",
    "timeEnv_prediction = pd.read_csv(\n",
    "    folder_path+f'TimeEnv/Predictions_epoch{num_epochs}.csv')\n",
    "print(freqEnv_actual.shape)\n",
    "print(freqEnv_prediction.shape)\n",
    "print(timeEnv_actual.shape)\n",
    "print(timeEnv_prediction.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# taking only the common RSN in both the actual and predicted data\n",
    "RSN_FreqEnv_actual = list(freqEnv_actual[\"RSN\"])\n",
    "RSN_TimeEnv_actual = list(timeEnv_actual[\"RSN\"])\n",
    "\n",
    "# Taking the common RSN in both the actual and predicted data\n",
    "common_RSN = list(set(RSN_FreqEnv_actual).intersection(\n",
    "    set(RSN_TimeEnv_actual)))\n",
    "\n",
    "freqEnv_actual = freqEnv_actual[freqEnv_actual[\"RSN\"].isin(common_RSN)]\n",
    "freqEnv_prediction = freqEnv_prediction[freqEnv_prediction[\"RSN\"].isin(\n",
    "    common_RSN)]\n",
    "timeEnv_actual = timeEnv_actual[timeEnv_actual[\"RSN\"].isin(common_RSN)]\n",
    "timeEnv_prediction = timeEnv_prediction[timeEnv_prediction[\"RSN\"].isin(\n",
    "    common_RSN)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shorting the data based on RSN\n",
    "freqEnv_actual = freqEnv_actual.sort_values(by=['RSN'])\n",
    "freqEnv_prediction = freqEnv_prediction.sort_values(by=['RSN'])\n",
    "timeEnv_actual = timeEnv_actual.sort_values(by=['RSN'])\n",
    "timeEnv_prediction = timeEnv_prediction.sort_values(by=['RSN'])\n",
    "\n",
    "\n",
    "# setting the RSN as index\n",
    "freqEnv_actual = freqEnv_actual.set_index('RSN')\n",
    "freqEnv_prediction = freqEnv_prediction.set_index('RSN')\n",
    "timeEnv_actual = timeEnv_actual.set_index('RSN')\n",
    "timeEnv_prediction = timeEnv_prediction.set_index('RSN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqEnv_actual\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Site_ID = freqEnv_actual.iloc[:, 0]\n",
    "Earthquake_ID = freqEnv_actual.iloc[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Total_RSN = list(freqEnv_actual.index)\n",
    "\n",
    "# req_no = 50\n",
    "# row_no = Total_RSN[:req_no]\n",
    "# row_no = [881.0, 1213.0, 1239.0, 1366.0, 1269.0, 1385.0,\n",
    "#           1551.0, 1578.0, 1592.0, 2212.0, 2735.0, 3179.0, 4540.0]\n",
    "\n",
    "row_no = Total_RSN\n",
    "\n",
    "actual_matrix = []\n",
    "prediction_matrix = []\n",
    "residuals_matrix = []\n",
    "freqEnv_actual_row = []\n",
    "freqEnv_prediction_row = []\n",
    "timeEnv_actual_row = []\n",
    "timeEnv_prediction_row = []\n",
    "\n",
    "for i in range(len(row_no)):\n",
    "    freqEnv_actual_row0 = freqEnv_actual.loc[row_no[i]][8:]\n",
    "    freqEnv_actual_row.append(\n",
    "        list(freqEnv_actual_row0.apply(lambda x: np.exp(x))))\n",
    "    freqEnv_prediction_row0 = freqEnv_prediction.loc[row_no[i]][8:]\n",
    "    freqEnv_prediction_row.append(\n",
    "        list(freqEnv_prediction_row0.apply(lambda x: np.exp(x))))\n",
    "    timeEnv_actual_row0 = timeEnv_actual.loc[row_no[i]][8:]\n",
    "    timeEnv_actual_row.append(\n",
    "        list(timeEnv_actual_row0.apply(lambda x: np.exp(x))))\n",
    "    timeEnv_prediction_row0 = timeEnv_prediction.loc[row_no[i]][8:]\n",
    "    timeEnv_prediction_row.append(\n",
    "        list(timeEnv_prediction_row0.apply(lambda x: np.exp(x))))\n",
    "    actual = np.outer(timeEnv_actual_row[i], freqEnv_actual_row[i])\n",
    "    prediction = np.outer(timeEnv_prediction_row[i], freqEnv_prediction_row[i])\n",
    "    actual_matrix.append(actual)\n",
    "    prediction_matrix.append(prediction)\n",
    "    residuals_matrix.append(prediction - actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(freqEnv_actual_row[0]))\n",
    "print(len(freqEnv_prediction_row[0]))\n",
    "print(len(timeEnv_actual_row[0]))\n",
    "print(len(timeEnv_prediction_row[0]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "acc_actual = []\n",
    "acc_prediction = []\n",
    "Ttot= int(len(timeEnv_actual_row[0])*0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the interval of time is 0.5 sec and freq 0.1 Hz\n",
    "#create the time vector having the interval of 0.5 sec and freq vector having the interval of 0.1 Hz \n",
    "timeArray = np.arange(0,len(timeEnv_actual_row[0])*0.5,0.5) # len of timeEnv_actual_row[0] is the length of the time envelope and is equal to 220\n",
    "freqArray = np.arange(0,len(freqEnv_actual_row[0])*0.1,0.1) # len of freqEnv_actual_row[0] is the length of the freq envelope and is equal to 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# converting the actual and the predicted time and frequency envelope to accleration time history\n",
    "# t = time, s ( in our it is the time envelope)\n",
    "# n = length of acc (say 80 for now)\n",
    "# f = Freq correspond to Epsd_freq envelopes, Hz  (in our case it is the frequency envelope)\n",
    "# XS_t_f = EPSD  (in our case it is the actual or predicted time envelope)\n",
    "# Ttot = Total time period, s (in our case it is lenght of time envelope *)\n",
    "\n",
    "def accleartion_time_history(t, n, f, XS_t_f, Ttot):\n",
    "    om_0 = 2 * np.pi / Ttot\n",
    "    ACC = np.zeros((len(t), n))\n",
    "    aa = 0\n",
    "    b = 2 * np.pi\n",
    "    tt = np.array(t)  # Ensure tt is a NumPy array\n",
    "    \n",
    "    for h in range(n):\n",
    "        r = (b - aa) * np.random.rand(len(f)) + aa\n",
    "        ACC[:, h] = np.zeros(len(t))\n",
    "        for kk in range(len(f)):\n",
    "            Cn = np.sqrt(2 * XS_t_f[:, kk])\n",
    "            cosin = np.cos((kk * om_0 * tt) + r[kk])\n",
    "            a_t = Cn * cosin\n",
    "            ACC[:, h] += a_t\n",
    "    \n",
    "    return ACC\n",
    "\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "for i in range(len(row_no)):\n",
    "    acc_actual.append(accleartion_time_history(timeArray, 1, freqArray, actual_matrix[i], Ttot))\n",
    "    acc_prediction.append(accleartion_time_history(timeArray, 1, freqArray, prediction_matrix[i], Ttot))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def responsespectrum(accel, ee, dt):\n",
    "    # ee - damping in % - 5 is recommended\n",
    "    # y - gamma in newmark's method - 0.5 is recommended\n",
    "    # b - beta in newmark's method - 0.25 is recommended\n",
    "    # td - time till which you want graph to be plotted\n",
    "\n",
    "    Tn = 10  # time period till which you want response spectrum\n",
    "    y = 0.5\n",
    "    b = 0.25\n",
    "    uo = 0\n",
    "    vo = 0\n",
    "    m = 1\n",
    "    z = ee / 100\n",
    "\n",
    "    na = len(accel)\n",
    "    nl = 2 * na\n",
    "    # T = np.array([0.01, 0.015, 0.02, 0.03, 0.04, 0.05, 0.06, 0.075, 0.09, 0.1,\n",
    "    #               0.15, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.75, 0.8, 0.9, 1, 1.2,\n",
    "    #               1.5, 2, 2.5, 3, 4, 5, 6, 7.5, 8, 9, 10])\n",
    "    T=np.array([0.010,\t0.020,\t0.022,\t0.025,\t0.029,\t0.030,\t0.032,\t0.035,\t0.036,\t0.040,\t0.042,\t0.044,\t0.045,\t0.046,\t0.048,\t0.050,\t0.055,\t0.060,\t0.065,\t0.067,\t0.070,\t0.075,\t0.080,\t0.085,\t0.090,\t0.095,\t0.100,\t0.110,\t0.120,\t0.130,\t0.133,\t0.140,\t0.150,\t0.160,\t0.170,\t0.180,\t0.190,\t0.200,\t0.220,\t0.240,\t0.250,\t0.260,\t0.280,\t0.290,\t0.300,\t0.320,\t0.340,\t0.350,\t0.360,\t0.380,\t0.400,\t0.420,\t0.440,\t0.450,\t0.460,\t0.480,\t0.500,\t0.550,\t0.600,\t0.650,\t0.667,\t0.700,\t0.750,\t0.800,\t0.850,\t0.900,\t0.950,\t1.000,\t1.100,\t1.200,\t1.300,\t1.400,\t1.500,\t1.600,\t1.700,\t1.800,\t1.900,\t2.000,\t2.200,\t2.400,\t2.500,\t2.600,\t2.800,\t3.000,\t3.200,\t3.400,\t3.500,\t3.600,\t3.800,\t4.000,\t4.200,\t4.400,\t4.600,\t4.800,\t5.000,\t5.500,\t6.000,\t6.500,\t7.000,\t7.500,\t8.000,\t8.500,\t9.000,\t9.500,\t10.000,\t11.000,\t12.000,\t13.000,\t14.000,\t15.000,\t20.000])\n",
    "\n",
    "    accel = np.concatenate((accel, np.zeros(nl - na)))\n",
    "    p = -m * accel\n",
    "\n",
    "    A = np.zeros(len(T))  # acceleration response spectrum - total acceleration\n",
    "    V = np.zeros(len(T))  # velocity response spectrum - relative velocity\n",
    "    D = np.zeros(len(T))  # displacement response spectrum - relative displacement\n",
    "\n",
    "    for j in range(len(T)):\n",
    "        fn = 1 / T[j]\n",
    "        wn = 2 * np.pi * fn\n",
    "        k = m * wn**2\n",
    "        c = 2 * m * wn * z\n",
    "\n",
    "        u = np.zeros(nl)\n",
    "        v = np.zeros(nl)\n",
    "        ac = np.zeros(nl)\n",
    "\n",
    "        u[0] = uo\n",
    "        v[0] = vo\n",
    "        ac[0] = (p[0] - c * vo - k * uo) / m\n",
    "\n",
    "        kf = k + y * c / (b * dt) + m / (b * dt**2)\n",
    "        a = m / (b * dt) + y * c / b\n",
    "        b2 = m / (2 * b) + dt * (y / (2 * b) - 1) * c\n",
    "\n",
    "        for i in range(nl - 1):\n",
    "            p1 = p[i]\n",
    "            p2 = p[i + 1]\n",
    "            dpf = (p2 - p1) + a * v[i] + b2 * ac[i]\n",
    "            du = dpf / kf\n",
    "            dv = y / (b * dt) * du - (y / b) * v[i] + dt * (1 - y / (2 * b)) * ac[i]\n",
    "            da = du / (b * dt**2) - v[i] / (b * dt) - ac[i] / (2 * b)\n",
    "            u[i + 1] = u[i] + du\n",
    "            v[i + 1] = v[i] + dv\n",
    "            ac[i + 1] = ac[i] + da\n",
    "\n",
    "        asd = ac + accel\n",
    "        A[j] = np.max(np.abs(asd))\n",
    "        V[j] = np.max(np.abs(v))\n",
    "        D[j] = np.max(np.abs(u))\n",
    "\n",
    "    A = np.concatenate(([np.max(np.abs(accel))], A))\n",
    "    V = np.concatenate(([0], V))\n",
    "    D = np.concatenate(([0], D))\n",
    "\n",
    "    PSV = (2 * np.pi / T) * D[1:]  # pseudo spectral velocity\n",
    "    PSV = np.concatenate(([PSV[0]], PSV))\n",
    "    PSA = ((2 * np.pi / T)**2) * D[1:]  # pseudo spectral acceleration\n",
    "    PSA = np.concatenate(([PSA[0]], PSA))\n",
    "    T = np.concatenate(([0], T))\n",
    "\n",
    "    return A, T\n",
    "\n",
    "# Example usage:\n",
    "# accel = np.random.randn(1000)  # Example acceleration data\n",
    "# ee = 5  # Damping percentage\n",
    "# dt = 0.02  # Time step\n",
    "# A, T = responsespectrum(accel, ee, dt)\n",
    "\n",
    "A_actual = []\n",
    "T_actual = []\n",
    "A_prediction = []\n",
    "T_prediction = []\n",
    "for i in range(len(row_no)):\n",
    "    A_actual0, T_actual0 = responsespectrum(acc_actual[i][:, 0], 5, 0.02)\n",
    "    A_actual.append(A_actual0)\n",
    "    T_actual.append(T_actual0)\n",
    "    A_prediction0, T_prediction0 = responsespectrum(acc_prediction[i][:, 0], 5, 0.02)\n",
    "    A_prediction.append(A_prediction0)\n",
    "    T_prediction.append(T_prediction0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reading the actual recorded response spectrum from nga west2\n",
    "nga_west2_path=root_folder+\"Program/Week6  Nerual Network/ngawest.csv\"\n",
    "nga_west2 = pd.read_csv(nga_west2_path,low_memory=False)\n",
    "\n",
    "\n",
    "#selecting only the common RSN in the actual recorded response spectrum and the predicted response spectrum\n",
    "nga_west2 = nga_west2[nga_west2[\"Record Sequence Number\"].isin(common_RSN)]\n",
    "nga_west2 = nga_west2.sort_values(by=['Record Sequence Number'])\n",
    "nga_west2 = nga_west2.set_index('Record Sequence Number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "# Function to extract the numeric part and convert it to float\n",
    "def extract_numeric_value(column_name):\n",
    "    match = re.search(r'^T(\\d+\\.\\d{3})S$', column_name)\n",
    "    if match:\n",
    "        return round(float(match.group(1)), 3)\n",
    "    return None\n",
    "\n",
    "# Extract the values from T_actual[0] and round them to 3 decimal places\n",
    "t_actual_values = {round(val, 3) for val in T_actual[0]}\n",
    "\n",
    "# Filter columns based on comparison with T_actual[0]\n",
    "filtered_columns = [col for col in nga_west2.columns if extract_numeric_value(col) in t_actual_values]\n",
    "\n",
    "# Select the filtered columns from the DataFrame\n",
    "selected_columns_nga_west2 = nga_west2[filtered_columns]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adding at starting a column containing 0 values to the selected_columns_nga_west2 to make the shape of the selected_columns_nga_west2 same as the shape of the T_actual[0]\n",
    "selected_columns_nga_west2.insert(0, 'T0.000S', 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculating the residuals of the response spectrum between the actual recorded response spectrum and actual response spectrum , actual recorded response spectrum and predicted response spectrum , actual response spectrum and predicted response spectrum\n",
    "residuals_rs_actual = []\n",
    "residuals_rs_prediction = []\n",
    "residuals_rs = []\n",
    "\n",
    "for i in range(len(row_no)):\n",
    "    residuals_rs_actual.append(A_actual[i] - selected_columns_nga_west2.loc[row_no[i]])\n",
    "    residuals_rs_prediction.append(A_prediction[i] - selected_columns_nga_west2.loc[row_no[i]])\n",
    "    residuals_rs.append(A_actual[i] - A_prediction[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a dataframe of the residuals of the response spectrum between the actual recorded response spectrum and predicted response spectrum \n",
    "residuals_rs_Predicted_df = pd.DataFrame(residuals_rs_prediction, index=row_no)\n",
    "# adding the first 8 columns of freqEnv_actual to the residuals_rs_Predicted_df\n",
    "residuals_rs_Predicted_df=pd.concat([pd.DataFrame(freqEnv_actual.iloc[:,0:8]), residuals_rs_Predicted_df],join='inner' ,axis=1)\n",
    "residuals_rs_Predicted_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_rs_Predicted_df.to_csv(folder_path+f'Residuals/Residuals_rs_Predicted_epoch{num_epochs}.csv', index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "residuals_rs_Predicted_df=pd.read_csv(folder_path+f'Residuals/Residuals_rs_Predicted_epoch{num_epochs}.csv',index_col=0)\n",
    "residuals_rs_Predicted_df\n",
    "col_val = residuals_rs_Predicted_df.columns\n",
    "Y_val=col_val[8:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all the unique values of the Earthquake_ID\n",
    "interval_means_interevents = []\n",
    "interval_stds_interevents = []\n",
    "unique_values =Earthquake_ID.unique()\n",
    "for i in range( unique_values.size):\n",
    "    Earthquake_ID_aligned, residuals_rs_Predicted_df_aligned = Earthquake_ID.align(residuals_rs_Predicted_df, join='inner')\n",
    "    mask = Earthquake_ID_aligned == unique_values[i]\n",
    "    interval_means_interevents.append(list(residuals_rs_Predicted_df.loc[mask].mean()))\n",
    "    interval_stds_interevents.append(list(residuals_rs_Predicted_df.loc[mask].std()))\n",
    "interval_means_intereventsdf=pd.DataFrame(interval_means_interevents,columns=col_val)\n",
    "interval_stds_intereventsdf=pd.DataFrame(interval_stds_interevents,columns=col_val)   \n",
    "interval_means_intereventsdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#getting all the unique values of the Site_ID\n",
    "interval_means_intraevents = []\n",
    "interval_stds_intraevents = []\n",
    "unique_values =Site_ID.unique()\n",
    "for i in range( unique_values.size):\n",
    "    Site_ID_aligned, residuals_rs_Predicted_df_aligned = Site_ID.align(residuals_rs_Predicted_df, join='inner')\n",
    "    mask = Site_ID_aligned == unique_values[i]\n",
    "    interval_means_intraevents.append(list(residuals_rs_Predicted_df.loc[mask].mean()))\n",
    "    interval_stds_intraevents.append(list(residuals_rs_Predicted_df.loc[mask].std()))\n",
    "interval_means_intraeventsdf=pd.DataFrame(interval_means_intraevents,columns=col_val)\n",
    "interval_stds_intraeventsdf=pd.DataFrame(interval_stds_intraevents,columns=col_val)  \n",
    "# interval_means_intraeventsdf[\"FreI_  7\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the error bar plot for the Residuals for each magnitude\n",
    "for i in range(len(Y_val)):\n",
    "    fig,(ax1,ax2)=plt.subplots(1,2,figsize=(25, 8))\n",
    "    ax1.errorbar(interval_means_intereventsdf[\"Mag\"], interval_means_intereventsdf[Y_val[i]], fmt='o', capsize=5, label='Mean Residual',color='red')\n",
    "    ax1.set_xlabel('Magnitude Values')\n",
    "    ax1.set_ylabel('Residuals')\n",
    "    ax1.set_title(f'Residual Plot for {Y_val[i]}')\n",
    "    ax1.grid(True)\n",
    "    ax2.errorbar(interval_means_intraeventsdf[\"Rjb\"], interval_means_intraeventsdf[Y_val[i]], fmt='o', capsize=5, label='Mean Residual',color='red')\n",
    "    ax2.set_xlabel('Rjb Values')\n",
    "    ax2.set_ylabel('Residuals')\n",
    "    ax2.set_title(f'Residual Plot for {Y_val[i]}')\n",
    "    ax2.grid(True)\n",
    "    # plt.savefig(f\"{root_folder}Program/Week8/PBNN/Figures/Inter_Inter_{Y_val[i]}_epoch{num_epochs}.png\")\n",
    "    plt.legend()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#taking 10 equal intervals between the minimum and maximum value of the x_test and finding the mean and standard deviation of the reSite_IDuals in each interval and plotting it\n",
    "colno=4\n",
    "x_test = residuals_rs_Predicted_df[col_val[colno]]\n",
    "num_intervals = 10\n",
    "interval_size = (x_test.max() - x_test.min()) / num_intervals\n",
    "interval_means = []\n",
    "interval_stds = []\n",
    "for i in range(num_intervals):\n",
    "    lower_bound = x_test.min() + i * interval_size\n",
    "    upper_bound = lower_bound + interval_size\n",
    "    mask = (residuals_rs_Predicted_df[col_val[colno]] >= lower_bound) & (residuals_rs_Predicted_df[col_val[colno]] <= upper_bound)\n",
    "    interval_means.append(list(residuals_rs_Predicted_df.loc[mask].mean()))\n",
    "    interval_stds.append(list(residuals_rs_Predicted_df.loc[mask].std()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_meansdf=pd.DataFrame(interval_means,columns=col_val)\n",
    "interval_stdsdf=pd.DataFrame(interval_stds,columns=col_val)\n",
    "interval_meansdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting reSite_IDual vs Rjb\n",
    "for i in range(len(Y_val)):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    plt.scatter(residuals_rs_Predicted_df[col_val[colno]], residuals_rs_Predicted_df[Y_val[i]], alpha=0.5, color='blue', label='Residuals')\n",
    "    plt.errorbar(np.arange(num_intervals) * interval_size + x_test.min(), interval_meansdf[f\"{Y_val[i]}\"], yerr=interval_stdsdf[f\"{Y_val[i]}\"], fmt='o', capsize=5, label='Mean Residual',color='red')\n",
    "    plt.xlabel('Rjb Values')\n",
    "    plt.ylabel('Residuals')\n",
    "    plt.title(f'Residuals Plot for {Y_val[i]}')\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    # plt.savefig(f\"{root_folder}Program/Week8/PBNN/Figures/Residuals_{Y_val[i]}_epoch{num_epochs}.png\")\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
