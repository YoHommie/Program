{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xsrc_0RuVDep"
      },
      "source": [
        "# Probabilistic Bayesian Neural Networks\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DIyKlXJuVDeu"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "Taking a probabilistic approach to deep learning allows to account for *uncertainty*,\n",
        "so that models can assign less levels of confidence to incorrect predictions.\n",
        "Sources of uncertainty can be found in the data, due to measurement error or\n",
        "noise in the labels, or the model, due to insufficient data availability for\n",
        "the model to learn effectively.\n",
        "\n",
        "\n",
        "This example demonstrates how to build basic probabilistic Bayesian neural networks\n",
        "to account for these two types of uncertainty.\n",
        "We use [TensorFlow Probability](https://www.tensorflow.org/probability) library,\n",
        "which is compatible with Keras API.\n",
        "\n",
        "This example requires TensorFlow 2.3 or higher.\n",
        "You can install Tensorflow Probability using the following command:\n",
        "\n",
        "```python\n",
        "pip install tensorflow-probability\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qw2PhEHaVDex"
      },
      "source": [
        "## The dataset\n",
        "\n",
        "We use the [Wine Quality](https://archive.ics.uci.edu/ml/datasets/wine+quality)\n",
        "dataset, which is available in the [TensorFlow Datasets](https://www.tensorflow.org/datasets/catalog/wine_quality).\n",
        "We use the red wine subset, which contains 4,898 examples.\n",
        "The dataset has 11numerical physicochemical features of the wine, and the task\n",
        "is to predict the wine quality, which is a score between 0 and 10.\n",
        "In this example, we treat this as a regression task.\n",
        "\n",
        "You can install TensorFlow Datasets using the following command:\n",
        "\n",
        "```python\n",
        "pip install tensorflow-datasets\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QWbXnqCsVDey"
      },
      "source": [
        "## Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "2ne46tLXqCrp"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "RSN\n",
            "0\n",
            "0.206896552\n",
            "0.413793103\n",
            "0.620689655\n",
            "0.827586207\n",
            "1.034482759\n",
            "1.24137931\n",
            "1.448275862\n",
            "1.655172414\n",
            "1.862068966\n",
            "2.068965517\n",
            "2.275862069\n",
            "2.482758621\n",
            "2.689655172\n",
            "2.896551724\n",
            "3.103448276\n",
            "3.310344828\n",
            "3.517241379\n",
            "3.724137931\n",
            "3.931034483\n",
            "4.137931034\n",
            "4.344827586\n",
            "4.551724138\n",
            "4.75862069\n",
            "4.965517241\n",
            "5.172413793\n",
            "5.379310345\n",
            "5.586206897\n",
            "5.793103448\n",
            "6\n",
            "Earthquake Magnitude\n",
            "Hypocenter Depth (km)\n",
            "Joyner-Boore Dist. (km)\n",
            "Vs30 (m/s) selected for analysis\n",
            "PGA (g)\n",
            "PGV (cm/sec)\n",
            "T0.010S\n",
            "T0.020S\n",
            "T0.022S\n",
            "T0.025S\n",
            "T0.029S\n",
            "T0.030S\n",
            "T0.032S\n",
            "T0.035S\n",
            "T0.036S\n",
            "T0.040S\n",
            "T0.042S\n",
            "T0.044S\n",
            "T0.045S\n",
            "T0.046S\n",
            "T0.048S\n",
            "T0.050S\n",
            "T0.055S\n",
            "T0.060S\n",
            "T0.065S\n",
            "T0.067S\n",
            "T0.070S\n",
            "T0.075S\n",
            "T0.080S\n",
            "T0.085S\n",
            "T0.090S\n",
            "T0.095S\n",
            "T0.100S\n",
            "T0.110S\n",
            "T0.120S\n",
            "T0.130S\n",
            "T0.133S\n",
            "T0.140S\n",
            "T0.150S\n",
            "T0.160S\n",
            "T0.170S\n",
            "T0.180S\n",
            "T0.190S\n",
            "T0.200S\n",
            "T0.220S\n",
            "T0.240S\n",
            "T0.250S\n",
            "T0.260S\n",
            "T0.280S\n",
            "T0.290S\n",
            "T0.300S\n",
            "T0.320S\n",
            "T0.340S\n",
            "T0.350S\n",
            "T0.360S\n",
            "T0.380S\n",
            "T0.400S\n",
            "T0.420S\n",
            "T0.440S\n",
            "T0.450S\n",
            "T0.460S\n",
            "T0.480S\n",
            "T0.500S\n",
            "T0.550S\n",
            "T0.600S\n",
            "T0.650S\n",
            "T0.667S\n",
            "T0.700S\n",
            "T0.750S\n",
            "T0.800S\n",
            "T0.850S\n",
            "T0.900S\n",
            "T0.950S\n",
            "T1.000S\n",
            "T1.100S\n",
            "T1.200S\n",
            "T1.300S\n",
            "T1.400S\n",
            "T1.500S\n",
            "T1.600S\n",
            "T1.700S\n",
            "T1.800S\n",
            "T1.900S\n",
            "T2.000S\n",
            "T2.200S\n",
            "T2.400S\n",
            "T2.500S\n",
            "T2.600S\n",
            "T2.800S\n",
            "T3.000S\n",
            "T3.200S\n",
            "T3.400S\n",
            "T3.500S\n",
            "T3.600S\n",
            "T3.800S\n",
            "T4.000S\n",
            "T4.200S\n",
            "T4.400S\n",
            "T4.600S\n",
            "T4.800S\n",
            "T5.000S\n",
            "T5.500S\n",
            "T6.000S\n",
            "T6.500S\n",
            "T7.000S\n",
            "T7.500S\n",
            "T8.000S\n",
            "T8.500S\n",
            "T9.000S\n",
            "T9.500S\n",
            "T10.000S\n",
            "T11.000S\n",
            "T12.000S\n",
            "T13.000S\n",
            "T14.000S\n",
            "T15.000S\n",
            "T20.000S\n",
            "RSN\n",
            "0\n",
            "0.206896552\n",
            "0.413793103\n",
            "0.620689655\n",
            "0.827586207\n",
            "1.034482759\n",
            "1.24137931\n",
            "1.448275862\n",
            "1.655172414\n",
            "1.862068966\n",
            "2.068965517\n",
            "2.275862069\n",
            "2.482758621\n",
            "2.689655172\n",
            "2.896551724\n",
            "3.103448276\n",
            "3.310344828\n",
            "3.517241379\n",
            "3.724137931\n",
            "3.931034483\n",
            "4.137931034\n",
            "4.344827586\n",
            "4.551724138\n",
            "4.75862069\n",
            "4.965517241\n",
            "5.172413793\n",
            "5.379310345\n",
            "5.586206897\n",
            "5.793103448\n",
            "6\n",
            "EM\n",
            "HD\n",
            "JBD\n",
            "Vs30\n",
            "PGA (g)\n",
            "PGV (cm/sec)\n",
            "T0.010S\n",
            "T0.020S\n",
            "T0.022S\n",
            "T0.025S\n",
            "T0.029S\n",
            "T0.030S\n",
            "T0.032S\n",
            "T0.035S\n",
            "T0.036S\n",
            "T0.040S\n",
            "T0.042S\n",
            "T0.044S\n",
            "T0.045S\n",
            "T0.046S\n",
            "T0.048S\n",
            "T0.050S\n",
            "T0.055S\n",
            "T0.060S\n",
            "T0.065S\n",
            "T0.067S\n",
            "T0.070S\n",
            "T0.075S\n",
            "T0.080S\n",
            "T0.085S\n",
            "T0.090S\n",
            "T0.095S\n",
            "T0.100S\n",
            "T0.110S\n",
            "T0.120S\n",
            "T0.130S\n",
            "T0.133S\n",
            "T0.140S\n",
            "T0.150S\n",
            "T0.160S\n",
            "T0.170S\n",
            "T0.180S\n",
            "T0.190S\n",
            "T0.200S\n",
            "T0.220S\n",
            "T0.240S\n",
            "T0.250S\n",
            "T0.260S\n",
            "T0.280S\n",
            "T0.290S\n",
            "T0.300S\n",
            "T0.320S\n",
            "T0.340S\n",
            "T0.350S\n",
            "T0.360S\n",
            "T0.380S\n",
            "T0.400S\n",
            "T0.420S\n",
            "T0.440S\n",
            "T0.450S\n",
            "T0.460S\n",
            "T0.480S\n",
            "T0.500S\n",
            "T0.550S\n",
            "T0.600S\n",
            "T0.650S\n",
            "T0.667S\n",
            "T0.700S\n",
            "T0.750S\n",
            "T0.800S\n",
            "T0.850S\n",
            "T0.900S\n",
            "T0.950S\n",
            "T1.000S\n",
            "T1.100S\n",
            "T1.200S\n",
            "T1.300S\n",
            "T1.400S\n",
            "T1.500S\n",
            "T1.600S\n",
            "T1.700S\n",
            "T1.800S\n",
            "T1.900S\n",
            "T2.000S\n",
            "T2.200S\n",
            "T2.400S\n",
            "T2.500S\n",
            "T2.600S\n",
            "T2.800S\n",
            "T3.000S\n",
            "T3.200S\n",
            "T3.400S\n",
            "T3.500S\n",
            "T3.600S\n",
            "T3.800S\n",
            "T4.000S\n",
            "T4.200S\n",
            "T4.400S\n",
            "T4.600S\n",
            "T4.800S\n",
            "T5.000S\n",
            "T5.500S\n",
            "T6.000S\n",
            "T6.500S\n",
            "T7.000S\n",
            "T7.500S\n",
            "T8.000S\n",
            "T8.500S\n",
            "T9.000S\n",
            "T9.500S\n",
            "T10.000S\n",
            "T11.000S\n",
            "T12.000S\n",
            "T13.000S\n",
            "T14.000S\n",
            "T15.000S\n",
            "T20.000S\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "\n",
        "dataframe= pd.read_csv(\"C:/Users/adity/OneDrive/Desktop/Sixth Semester/CE6018 Seismic Data Analytics/Program/Week7/ReqResampleData/req_time_x.csv\")\n",
        "dataframe.drop(dataframe[(dataframe['T0.010S'] == -999)].index, inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "# import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "\n",
        "# req_cols=[\"Earthquake Magnitude\",\"Hypocenter Depth (km)\",\"Joyner-Boore Dist. (km)\",\"Vs30 (m/s) selected for analysis\"]\n",
        "# features = {column: tf.constant(dataframe[column].values) for column in req_cols}\n",
        "# target =dataframe['T0.010S']\n",
        "# # Convert Pandas DataFrame to TensorFlow Dataset\n",
        "# dataset = tf.data.Dataset.from_tensor_slices((features, target))\n",
        "# # Shuffle and batch the dataset\n",
        "# dataset = dataset.shuffle(buffer_size=len(dataframe)).batch(256)\n",
        "# # Split into training and test datasets\n",
        "# data_size= len(dataset)\n",
        "# train_dataset = dataset.take(int(data_size*0.85))\n",
        "# test_dataset = dataset.skip(int(data_size*0.15))\n",
        "# # for i in range(len(dataframe.columns)):\n",
        "\n",
        "\n",
        "# print(type(train_dataset))\n",
        "# print(type(test_dataset))\n",
        "# print(list(train_dataset.as_numpy_iterator()))\n",
        "# print(list(test_dataset.as_numpy_iterator()))\n",
        "# #  print(dataframe.columns[i])\n",
        "\n",
        "\n",
        "# dataset_size = 4898\n",
        "# batch_size = 256\n",
        "# train_size = int(dataset_size * 0.85)\n",
        "\n",
        "\n",
        "\n",
        "# #  We prefetch with a buffer the same size as the dataset because th dataset\n",
        "#     # is very small and fits into memory.\n",
        "# dataset = (\n",
        "#     tfds.load(name=\"wine_quality\", as_supervised=True, split=\"train\")\n",
        "#     .map(lambda x, y: (x, tf.cast(y, tf.float32)))\n",
        "#     .prefetch(buffer_size=dataset_size)\n",
        "#     .cache()\n",
        "# )\n",
        "# # We shuffle with a buffer the same size as the dataset.\n",
        "# train_dataset = (\n",
        "#     dataset.take(train_size).shuffle(buffer_size=train_size).batch(batch_size)\n",
        "# )\n",
        "# test_dataset = dataset.skip(train_size).batch(batch_size)\n",
        "\n",
        "\n",
        "\n",
        "# print(type(train_dataset))\n",
        "# print(type(test_dataset))\n",
        "# print(list(train_dataset.as_numpy_iterator()))\n",
        "# print(list(test_dataset.as_numpy_iterator()))\n",
        "\n",
        "for i in dataframe.columns:\n",
        "    print(i)\n",
        "\n",
        "dataframe.rename(columns={\n",
        "\"Earthquake Magnitude\":\"EM\",\n",
        "\"Hypocenter Depth (km)\": \"HD\",\n",
        "\"Joyner-Boore Dist. (km)\": \"JBD\",\n",
        "\"Vs30 (m/s) selected for analysis\": \"Vs30\",\n",
        "}, inplace=True)\n",
        "\n",
        "for i in dataframe.columns:\n",
        "    print(i)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "M-V0zA5LHuEt"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# req_cols=[\"Earthquake Magnitude\",\"Hypocenter Depth (km)\",\"Joyner-Boore Dist. (km)\",\"Vs30 (m/s) selected for analysis\"]\n",
        "# features = {column: tf.constant(dataframe[column].values) for column in req_cols}\n",
        "# target =dataframe['T0.010S']\n",
        "# # Convert Pandas DataFrame to TensorFlow Dataset\n",
        "# dataset = tf.data.Dataset.from_tensor_slices((features, target))\n",
        "# # Shuffle and batch the dataset\n",
        "# dataset = dataset.shuffle(buffer_size=len(dataframe)).batch(256)\n",
        "# # Split into training and test datasets\n",
        "# data_size= len(dataset)\n",
        "# train_dataset = dataset.take(int(data_size*0.85))\n",
        "# test_dataset = dataset.skip(int(data_size*0.15))\n",
        "# # for i in range(len(dataframe.columns)):\n",
        "\n",
        "\n",
        "# print(type(train_dataset))\n",
        "# print(type(test_dataset))\n",
        "# print(list(train_dataset.as_numpy_iterator()))\n",
        "# print(list(test_dataset.as_numpy_iterator()))\n",
        "# #  print(dataframe.columns[i])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "kzW_YEGgVDez"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "# import tensorflow_datasets as tfds\n",
        "import tensorflow_probability as tfp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b5k3-4NnVDe0"
      },
      "source": [
        "## Create training and evaluation datasets\n",
        "\n",
        "Here, we load the `wine_quality` dataset using `tfds.load()`, and we convert\n",
        "the target feature to float. Then, we shuffle the dataset and split it into\n",
        "training and test sets. We take the first `train_size` examples as the train\n",
        "split, and the rest as the test split."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "hqQ1eNP8VDe1"
      },
      "outputs": [],
      "source": [
        "\n",
        "# def get_train_and_test_splits(train_size, batch_size=1):\n",
        "#     # We prefetch with a buffer the same size as the dataset because th dataset\n",
        "#     # is very small and fits into memory.\n",
        "#     dataset = (\n",
        "#         tfds.load(name=\"wine_quality\", as_supervised=True, split=\"train\")\n",
        "#         .map(lambda x, y: (x, tf.cast(y, tf.float32)))\n",
        "#         .prefetch(buffer_size=dataset_size)\n",
        "#         .cache()\n",
        "#     )\n",
        "#     # We shuffle with a buffer the same size as the dataset.\n",
        "#     train_dataset = (\n",
        "#         dataset.take(train_size).shuffle(buffer_size=train_size).batch(batch_size)\n",
        "#     )\n",
        "#     test_dataset = dataset.skip(train_size).batch(batch_size)\n",
        "\n",
        "#     return train_dataset, test_dataset\n",
        "\n",
        "########################### For creating the TensorFlow dataset\n",
        "\n",
        "# import tensorflow as tf\n",
        "# import pandas as pd\n",
        "\n",
        "# # Assuming 'df' is your pandas DataFrame containing the dataset\n",
        "# # Example DataFrame\n",
        "# df = pd.DataFrame({\n",
        "#     'alcohol': [12.3, 10.1, 11.5],\n",
        "#     'chlorides': [0.045, 0.035, 0.04],\n",
        "#     'citric acid': [0.3, 0.25, 0.28],\n",
        "#     'density': [0.998, 0.995, 0.997],\n",
        "#     'fixed acidity': [7.4, 7.8, 7.6],\n",
        "#     'free sulfur dioxide': [11, 25, 15],\n",
        "#     'pH': [3.51, 3.2, 3.26],\n",
        "#     'residual sugar': [1.9, 2.6, 2.3],\n",
        "#     'sulphates': [0.47, 0.52, 0.5],\n",
        "#     'total sulfur dioxide': [34, 67, 54],\n",
        "#     'volatile acidity': [0.7, 0.88, 0.76],\n",
        "# })\n",
        "\n",
        "# # Convert DataFrame columns to tensors\n",
        "# features = {column: tf.constant(df[column].values) for column in df.columns}\n",
        "# labels = tf.constant([0, 1, 0])  # Example labels\n",
        "\n",
        "# # Create a dataset from tensors\n",
        "# dataset = tf.data.Dataset.from_tensor_slices((features, labels))\n",
        "\n",
        "# # Print the element_spec of the dataset\n",
        "# print(dataset.element_spec)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "#///////////////////////////////////////////////////////////\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_train_and_test_splits(train_size, batch_size=1):\n",
        "    # Split features and target\n",
        "\n",
        "    req_cols=[\"EM\",\"HD\",\"JBD\",\"Vs30\"]\n",
        "    features = {column: tf.constant(dataframe[column].values) for column in req_cols}\n",
        "    # target =dataframe['T0.010S']\n",
        "    target ={\"T0.010S\":tf.constant(dataframe['T0.010S'].values)}\n",
        "\n",
        "    # Convert Pandas DataFrame to TensorFlow Dataset\n",
        "    dataset = tf.data.Dataset.from_tensor_slices((features, target))\n",
        "\n",
        "    # Shuffle and batch the dataset\n",
        "    dataset = dataset.shuffle(buffer_size=len(dataframe)).batch(batch_size)\n",
        "\n",
        "    # Split into training and test datasets\n",
        "    train_dataset = dataset.take(train_size)\n",
        "    test_dataset = dataset.skip(train_size)\n",
        "\n",
        "    return train_dataset, test_dataset\n",
        "\n",
        "# Example usage:\n",
        "# Assuming you have a Pandas DataFrame named 'df' containing your data\n",
        "# train_size = int(len(df) * 0.7)  # 70% for training, 30% for testing\n",
        "# batch_size = 32  # You can adjust batch size as needed\n",
        "\n",
        "# train_dataset, test_dataset = get_train_and_test_splits_from_dataframe(df, train_size, batch_size)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bZrhgQb3VDe1"
      },
      "source": [
        "## Compile, train, and evaluate the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "HJ_S7jTFVDe1"
      },
      "outputs": [],
      "source": [
        "hidden_units = [8, 8]\n",
        "learning_rate = 0.001\n",
        "\n",
        "\n",
        "def run_experiment(model, loss, train_dataset, test_dataset):\n",
        "\n",
        "    model.compile(\n",
        "        optimizer=keras.optimizers.RMSprop(learning_rate=learning_rate),\n",
        "        loss=keras.losses.MeanSquaredError(),\n",
        "        metrics=[keras.metrics.RootMeanSquaredError()],\n",
        "    )\n",
        "\n",
        "    print(\"Start training the model...\")\n",
        "    model.fit(train_dataset, epochs=num_epochs, validation_data=test_dataset)\n",
        "    print(\"Model training finished.\")\n",
        "    _, rmse = model.evaluate(train_dataset, verbose=0)\n",
        "    print(f\"Train RMSE: {round(rmse, 3)}\")\n",
        "\n",
        "    print(\"Evaluating model performance...\")\n",
        "    _, rmse = model.evaluate(test_dataset, verbose=0)\n",
        "    print(f\"Test RMSE: {round(rmse, 3)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1PQJV9eVDe2"
      },
      "source": [
        "## Create model inputs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "Cr1zKPwbVDe2"
      },
      "outputs": [],
      "source": [
        "FEATURE_NAMES = [\"EM\", \"HD\", \"JBD\", \"Vs30\"]\n",
        "\n",
        "\n",
        "def create_model_inputs():\n",
        "    inputs = {}\n",
        "    for feature_name in FEATURE_NAMES:\n",
        "        inputs[feature_name] = layers.Input(\n",
        "            name=feature_name, shape=(1,), dtype=tf.float32\n",
        "        )\n",
        "    return inputs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DVdtTZy2VDe2"
      },
      "source": [
        "## Experiment 1: standard neural network\n",
        "\n",
        "We create a standard deterministic neural network model as a baseline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "CH7ZZfWGVDe2"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_baseline_model():\n",
        "    inputs = create_model_inputs()\n",
        "    input_values = [value for _, value in sorted(inputs.items())]\n",
        "    features = keras.layers.concatenate(input_values)\n",
        "    features = layers.BatchNormalization()(features)\n",
        "\n",
        "    # Create hidden layers with deterministic weights using the Dense layer.\n",
        "    for units in hidden_units:\n",
        "        features = layers.Dense(units, activation=\"sigmoid\")(features)\n",
        "    # The output is deterministic: a single point estimate.\n",
        "    outputs = layers.Dense(units=1)(features)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UYnYuI9AVDe3"
      },
      "source": [
        "Let's split the wine dataset into training and test sets, with 85% and 15% of\n",
        "the examples, respectively."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "FHDpHE9NVDe3"
      },
      "outputs": [],
      "source": [
        "dataset_size = len(dataframe)\n",
        "batch_size = 256\n",
        "train_size = int(dataset_size * 0.85)\n",
        "train_dataset, test_dataset = get_train_and_test_splits(train_size, batch_size)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UnbnDaxQVDe3"
      },
      "source": [
        "Now let's train the baseline model. We use the `MeanSquaredError`\n",
        "as the loss function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        },
        "id": "9eDw7yj_VDe3",
        "outputId": "4a7d5a7e-ccfc-4f08-8a4e-abc82b548c54"
      },
      "outputs": [],
      "source": [
        "num_epochs = 100\n",
        "mse_loss = keras.losses.MeanSquaredError()\n",
        "baseline_model = create_baseline_model()\n",
        "\n",
        "# Ensure that a loss function is provided when compiling the model\n",
        "baseline_model.compile(loss=mse_loss)\n",
        "\n",
        "# run_experiment(baseline_model, mse_loss, train_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f1Un0HBaVDe3"
      },
      "source": [
        "We take a sample from the test set use the model to obtain predictions for them.\n",
        "Note that since the baseline model is deterministic, we get a single a\n",
        "*point estimate* prediction for each test example, with no information about the\n",
        "uncertainty of the model nor the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "s6Hmu1p6VDe3"
      },
      "outputs": [],
      "source": [
        "# sample = 10\n",
        "# examples, targets = list(test_dataset.unbatch().shuffle(batch_size * 10).batch(sample))[\n",
        "#     0\n",
        "# ]\n",
        "\n",
        "# predicted = baseline_model(examples).numpy()\n",
        "# for idx in range(sample):\n",
        "#     print(f\"Predicted: {round(float(predicted[idx][0]), 1)} - Actual: {targets[idx]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rlbk1wWbVDe3"
      },
      "source": [
        "## Experiment 2: Bayesian neural network (BNN)\n",
        "\n",
        "The object of the Bayesian approach for modeling neural networks is to capture\n",
        "the *epistemic uncertainty*, which is uncertainty about the model fitness,\n",
        "due to limited training data.\n",
        "\n",
        "The idea is that, instead of learning specific weight (and bias) *values* in the\n",
        "neural network, the Bayesian approach learns weight *distributions*\n",
        "- from which we can sample to produce an output for a given input -\n",
        "to encode weight uncertainty.\n",
        "\n",
        "Thus, we need to define prior and the posterior distributions of these weights,\n",
        "and the training process is to learn the parameters of these distributions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "tTSvzX1BVDe4"
      },
      "outputs": [],
      "source": [
        "# Define the prior weight distribution as Normal of mean=0 and stddev=1.\n",
        "# Note that, in this example, the we prior distribution is not trainable,\n",
        "# as we fix its parameters.\n",
        "def prior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    prior_model = keras.Sequential(\n",
        "        [\n",
        "            tfp.layers.DistributionLambda(\n",
        "                lambda t: tfp.distributions.MultivariateNormalDiag(\n",
        "                    loc=tf.zeros(n), scale_diag=tf.ones(n)\n",
        "                )\n",
        "            )\n",
        "        ]\n",
        "    )\n",
        "    return prior_model\n",
        "\n",
        "\n",
        "# Define variational posterior weight distribution as multivariate Gaussian.\n",
        "# Note that the learnable parameters for this distribution are the means,\n",
        "# variances, and covariances.\n",
        "def posterior(kernel_size, bias_size, dtype=None):\n",
        "    n = kernel_size + bias_size\n",
        "    posterior_model = keras.Sequential(\n",
        "        [\n",
        "            tfp.layers.VariableLayer(\n",
        "                tfp.layers.MultivariateNormalTriL.params_size(n), dtype=dtype\n",
        "            ),\n",
        "            tfp.layers.MultivariateNormalTriL(n),\n",
        "        ]\n",
        "    )\n",
        "    return posterior_model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oz6ti4yaVDe4"
      },
      "source": [
        "We use the `tfp.layers.DenseVariational` layer instead of the standard\n",
        "`keras.layers.Dense` layer in the neural network model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "sivGUapwVDe4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_bnn_model(train_size):\n",
        "    inputs = create_model_inputs()\n",
        "    features = keras.layers.concatenate(list(inputs.values()))\n",
        "    features = layers.BatchNormalization()(features)\n",
        "\n",
        "    # Create hidden layers with weight uncertainty using the DenseVariational layer.\n",
        "    for units in hidden_units:\n",
        "        features = tfp.layers.DenseVariational(\n",
        "            units=units,\n",
        "            make_prior_fn=prior,\n",
        "            make_posterior_fn=posterior,\n",
        "            kl_weight=1 / train_size,\n",
        "            activation=\"sigmoid\",\n",
        "        )(features)\n",
        "\n",
        "    # The output is deterministic: a single point estimate.\n",
        "    outputs = layers.Dense(units=1)(features)\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "828H7_bFVDe4"
      },
      "source": [
        "The epistemic uncertainty can be reduced as we increase the size of the\n",
        "training data. That is, the more data the BNN model sees, the more it is certain\n",
        "about its estimates for the weights (distribution parameters).\n",
        "Let's test this behaviour by training the BNN model on a small subset of\n",
        "the training set, and then on the full training set, to compare the output variances."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "maUc6rZRVDe4"
      },
      "source": [
        "### Train BNN  with a small training subset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "-d4YGqgJVDe4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:From c:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\backend.py:873: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n"
          ]
        },
        {
          "ename": "AttributeError",
          "evalue": "'tuple' object has no attribute 'rank'",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m train_sample_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(train_size \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m0.3\u001b[39m)\n\u001b[0;32m      3\u001b[0m small_train_dataset \u001b[38;5;241m=\u001b[39m train_dataset\u001b[38;5;241m.\u001b[39munbatch()\u001b[38;5;241m.\u001b[39mtake(train_sample_size)\u001b[38;5;241m.\u001b[39mbatch(batch_size)\n\u001b[1;32m----> 5\u001b[0m bnn_model_small \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_bnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_sample_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m run_experiment(bnn_model_small, mse_loss, small_train_dataset, test_dataset)\n",
            "Cell \u001b[1;32mIn[21], line 8\u001b[0m, in \u001b[0;36mcreate_bnn_model\u001b[1;34m(train_size)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Create hidden layers with weight uncertainty using the DenseVariational layer.\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m units \u001b[38;5;129;01min\u001b[39;00m hidden_units:\n\u001b[1;32m----> 8\u001b[0m     features \u001b[38;5;241m=\u001b[39m \u001b[43mtfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDenseVariational\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m        \u001b[49m\u001b[43munits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmake_prior_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmake_posterior_fn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mposterior\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     12\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkl_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mtrain_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     13\u001b[0m \u001b[43m        \u001b[49m\u001b[43mactivation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msigmoid\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m     14\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# The output is deterministic: a single point estimate.\u001b[39;00m\n\u001b[0;32m     17\u001b[0m outputs \u001b[38;5;241m=\u001b[39m layers\u001b[38;5;241m.\u001b[39mDense(units\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)(features)\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
            "File \u001b[1;32mc:\\Users\\adity\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tf_keras\\src\\engine\\input_spec.py:251\u001b[0m, in \u001b[0;36massert_input_compatibility\u001b[1;34m(input_spec, inputs, layer_name)\u001b[0m\n\u001b[0;32m    244\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    245\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    246\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    247\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexpected max_ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec\u001b[38;5;241m.\u001b[39mmax_ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    248\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfound ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    249\u001b[0m         )\n\u001b[0;32m    250\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m spec\u001b[38;5;241m.\u001b[39mmin_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 251\u001b[0m     ndim \u001b[38;5;241m=\u001b[39m \u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrank\u001b[49m\n\u001b[0;32m    252\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim \u001b[38;5;241m<\u001b[39m spec\u001b[38;5;241m.\u001b[39mmin_ndim:\n\u001b[0;32m    253\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    254\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m of layer \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    255\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mis incompatible with the layer: \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtuple\u001b[39m(shape)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    259\u001b[0m         )\n",
            "\u001b[1;31mAttributeError\u001b[0m: 'tuple' object has no attribute 'rank'"
          ]
        }
      ],
      "source": [
        "num_epochs = 500\n",
        "train_sample_size = int(train_size * 0.3)\n",
        "small_train_dataset = train_dataset.unbatch().take(train_sample_size).batch(batch_size)\n",
        "\n",
        "bnn_model_small = create_bnn_model(train_sample_size)\n",
        "run_experiment(bnn_model_small, mse_loss, small_train_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LlLlwWDjVDe4"
      },
      "source": [
        "Since we have trained a BNN model, the model produces a different output each time\n",
        "we call it with the same input, since each time a new set of weights are sampled\n",
        "from the distributions to construct the network and produce an output.\n",
        "The less certain the mode weights are, the more variability (wider range) we will\n",
        "see in the outputs of the same inputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1Yym8y-wVDe4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def compute_predictions(model, iterations=100):\n",
        "    predicted = []\n",
        "    for _ in range(iterations):\n",
        "        predicted.append(model(examples).numpy())\n",
        "    predicted = np.concatenate(predicted, axis=1)\n",
        "\n",
        "    prediction_mean = np.mean(predicted, axis=1).tolist()\n",
        "    prediction_min = np.min(predicted, axis=1).tolist()\n",
        "    prediction_max = np.max(predicted, axis=1).tolist()\n",
        "    prediction_range = (np.max(predicted, axis=1) - np.min(predicted, axis=1)).tolist()\n",
        "\n",
        "    for idx in range(sample):\n",
        "        print(\n",
        "            f\"Predictions mean: {round(prediction_mean[idx], 2)}, \"\n",
        "            f\"min: {round(prediction_min[idx], 2)}, \"\n",
        "            f\"max: {round(prediction_max[idx], 2)}, \"\n",
        "            f\"range: {round(prediction_range[idx], 2)} - \"\n",
        "            f\"Actual: {targets[idx]}\"\n",
        "        )\n",
        "\n",
        "\n",
        "compute_predictions(bnn_model_small)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QKpcxjVcVDe5"
      },
      "source": [
        "### Train BNN  with the whole training set."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8E_yMHOaVDe5"
      },
      "outputs": [],
      "source": [
        "num_epochs = 500\n",
        "bnn_model_full = create_bnn_model(train_size)\n",
        "run_experiment(bnn_model_full, mse_loss, train_dataset, test_dataset)\n",
        "\n",
        "compute_predictions(bnn_model_full)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEoj9tJgVDe5"
      },
      "source": [
        "Notice that the model trained with the full training dataset shows smaller range\n",
        "(uncertainty) in the prediction values for the same inputs, compared to the model\n",
        "trained with a subset of the training dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pw57RqAjVDe5"
      },
      "source": [
        "## Experiment 3: probabilistic Bayesian neural network\n",
        "\n",
        "So far, the output of the standard and the Bayesian NN models that we built is\n",
        "deterministic, that is, produces a point estimate as a prediction for a given example.\n",
        "We can create a probabilistic NN by letting the model output a distribution.\n",
        "In this case, the model captures the *aleatoric uncertainty* as well,\n",
        "which is due to irreducible noise in the data, or to the stochastic nature of the\n",
        "process generating the data.\n",
        "\n",
        "In this example, we model the output as a `IndependentNormal` distribution,\n",
        "with learnable mean and variance parameters. If the task was classification,\n",
        "we would have used `IndependentBernoulli` with binary classes, and `OneHotCategorical`\n",
        "with multiple classes, to model distribution of the model output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yAoYFxXfVDe5"
      },
      "outputs": [],
      "source": [
        "\n",
        "def create_probablistic_bnn_model(train_size):\n",
        "    inputs = create_model_inputs()\n",
        "    features = keras.layers.concatenate(list(inputs.values()))\n",
        "    features = layers.BatchNormalization()(features)\n",
        "\n",
        "    # Create hidden layers with weight uncertainty using the DenseVariational layer.\n",
        "    for units in hidden_units:\n",
        "        features = tfp.layers.DenseVariational(\n",
        "            units=units,\n",
        "            make_prior_fn=prior,\n",
        "            make_posterior_fn=posterior,\n",
        "            kl_weight=1 / train_size,\n",
        "            activation=\"sigmoid\",\n",
        "        )(features)\n",
        "\n",
        "    # Create a probabilisticå output (Normal distribution), and use the `Dense` layer\n",
        "    # to produce the parameters of the distribution.\n",
        "    # We set units=2 to learn both the mean and the variance of the Normal distribution.\n",
        "    distribution_params = layers.Dense(units=2)(features)\n",
        "    outputs = tfp.layers.IndependentNormal(1)(distribution_params)\n",
        "\n",
        "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
        "    return model\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bmenigwcVDe5"
      },
      "source": [
        "Since the output of the model is a distribution, rather than a point estimate,\n",
        "we use the [negative loglikelihood](https://en.wikipedia.org/wiki/Likelihood_function)\n",
        "as our loss function to compute how likely to see the true data (targets) from the\n",
        "estimated distribution produced by the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "btXn_5imVDe6"
      },
      "outputs": [],
      "source": [
        "\n",
        "def negative_loglikelihood(targets, estimated_distribution):\n",
        "    return -estimated_distribution.log_prob(targets)\n",
        "\n",
        "\n",
        "num_epochs = 1000\n",
        "prob_bnn_model = create_probablistic_bnn_model(train_size)\n",
        "run_experiment(prob_bnn_model, negative_loglikelihood, train_dataset, test_dataset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCX0Z5TgVDe6"
      },
      "source": [
        "Now let's produce an output from the model given the test examples.\n",
        "The output is now a distribution, and we can use its mean and variance\n",
        "to compute the confidence intervals (CI) of the prediction."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eOGOZugfVDe6"
      },
      "outputs": [],
      "source": [
        "prediction_distribution = prob_bnn_model(examples)\n",
        "prediction_mean = prediction_distribution.mean().numpy().tolist()\n",
        "prediction_stdv = prediction_distribution.stddev().numpy()\n",
        "\n",
        "# The 95% CI is computed as mean ± (1.96 * stdv)\n",
        "upper = (prediction_mean + (1.96 * prediction_stdv)).tolist()\n",
        "lower = (prediction_mean - (1.96 * prediction_stdv)).tolist()\n",
        "prediction_stdv = prediction_stdv.tolist()\n",
        "\n",
        "for idx in range(sample):\n",
        "    print(\n",
        "        f\"Prediction mean: {round(prediction_mean[idx][0], 2)}, \"\n",
        "        f\"stddev: {round(prediction_stdv[idx][0], 2)}, \"\n",
        "        f\"95% CI: [{round(upper[idx][0], 2)} - {round(lower[idx][0], 2)}]\"\n",
        "        f\" - Actual: {targets[idx]}\"\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
